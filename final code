#<데이터 불러오는거>

import os
import tensorflow as tf

# 1) 데이터가 위치한 절대 경로
base_path = r"C:/Users/taedo/Downloads/intern_data/lumbar_spinal_dataset"
train_dir = os.path.join(base_path, 'training')
test_dir  = os.path.join(base_path, 'testing')

# 2) 경로 확인 (옵션)
for d in (train_dir, test_dir):
    print(d, "→", "Exists?" , os.path.exists(d), " Samples:", os.listdir(d)[:3])

# 3) 데이터셋 생성
train_ds = tf.keras.utils.image_dataset_from_directory(
    train_dir,
    labels='inferred',
    label_mode='int',
    image_size=(224,224),
    batch_size=32,
    shuffle=True,
    seed=42
)

test_ds = tf.keras.utils.image_dataset_from_directory(
    test_dir,
    labels='inferred',
    label_mode='int',
    image_size=(224,224),
    batch_size=32,
    shuffle=False
)

# 4) 클래스 이름 출력
print("Classes:", train_ds.class_names)

# <불러오는거>
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import load_img, img_to_array
import cv2
import numpy as np
import tensorflow as tf


#<전처리 / 라벨링>
import os
import cv2
import numpy as np
from pathlib import Path
from tqdm.auto import tqdm

# ----------------------------------------------------
# 0) 전역 설정
# ----------------------------------------------------
IMG_SIZE   = (224, 224)
SRC_ROOT   = Path(r"C:/Users/taedo/Downloads/intern_data/lumbar_spinal_dataset")
DST_ROOT   = Path(r"C:/Users/taedo/Downloads/intern_data/lumbar_spinal_dataset/data_preprocessed_labeled")
SPLITS     = ['training', 'testing']
CLASSES    = ['Thecal Sac', 'No Stenosis', 'Herniated Disc']

# ----------------------------------------------------
# 1) 총 이미지 수 계산
# ----------------------------------------------------
total_imgs = 0
for split in SPLITS:
    for cls in CLASSES:
        total_imgs += len(list((SRC_ROOT / split / cls).glob('*.*')))

# ----------------------------------------------------
# 2) 전처리 함수 (기존과 동일)
# ----------------------------------------------------
def preprocess_knee(img_path: str) -> np.ndarray:
    gray      = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    denoised  = cv2.bilateralFilter(gray,  d=9, sigmaColor=75, sigmaSpace=75)
    clahe     = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    equalized = clahe.apply(denoised)

    t         = int(equalized.mean())
    _, mask   = cv2.threshold(equalized, t, 255, cv2.THRESH_BINARY)
    kernel    = np.ones((5,5), np.uint8)
    mask      = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=2)
    mask      = cv2.morphologyEx(mask, cv2.MORPH_OPEN,  kernel, iterations=1)

    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if contours:
        c        = max(contours, key=cv2.contourArea)
        x, y, w, h = cv2.boundingRect(c)
        cropped  = equalized[y:y+h, x:x+w]
    else:
        cropped  = equalized

    resized = cv2.resize(cropped, IMG_SIZE, interpolation=cv2.INTER_AREA)
    rgb     = cv2.cvtColor(resized, cv2.COLOR_GRAY2RGB)
    return (rgb * 255).astype(np.uint8)

# ----------------------------------------------------
# 3) 전처리 + 라벨링 + 진행바
# ----------------------------------------------------
DST_ROOT.mkdir(parents=True, exist_ok=True)
pbar = tqdm(total=total_imgs, desc="Processing images", unit="img")

for split in SPLITS:
    for cls in CLASSES:
        src_dir = SRC_ROOT / split / cls
        dst_dir = DST_ROOT / split / cls
        dst_dir.mkdir(parents=True, exist_ok=True)

        for img_path in src_dir.glob('*.*'):
            # 전처리
            proc = preprocess_knee(str(img_path))
            # 라벨 오버레이
            cv2.putText(
                proc, f"Class {cls}", (10,30),
                cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,255,0), 2, cv2.LINE_AA
            )
            # 저장
            cv2.imwrite(str(dst_dir / img_path.name), proc)
            # 진행바 업데이트
            pbar.update(1)

pbar.close()
print("✅ 전처리 & 라벨링 완료:", DST_ROOT)


#<라벨링된 데이터 확인>
import os
import random
from PIL import Image
import matplotlib.pyplot as plt

# 1) 실제 전처리된 폴더 경로 지정
ROOT_DIR = r"C:/Users/taedo/Downloads/intern_data/lumbar_spinal_dataset/data_preprocessed_labeled"  # ← 이 경로를 실제 위치로 바꿔주세요
TRAIN_DIR = os.path.join(ROOT_DIR, "training")

# 2) 클래스 디렉터리 자동 감지
classes = sorted([d for d in os.listdir(TRAIN_DIR)
                  if os.path.isdir(os.path.join(TRAIN_DIR, d))])
print("Found classes:", classes)

# 3) 샘플링할 이미지 목록 만들기
samples = []
for cls in classes:
    cls_dir = os.path.join(train_dir, cls)
    fnames = [f for f in os.listdir(cls_dir)
              if f.lower().endswith((".png",".jpg",".jpeg"))]
    # 각 클래스당 하나씩, 최대 9장
    samples += [(os.path.join(cls_dir, random.choice(fnames)), cls)]
    if len(samples) >= 9:
        break

# 4) 3×3 그리드로 시각화
plt.figure(figsize=(8,8))
for i,(img_path, cls) in enumerate(samples[:9]):
    img = Image.open(img_path).convert("L")  # 그레이스케일로 읽음
    ax = plt.subplot(3,3,i+1)
    plt.imshow(img, cmap="gray")
    plt.title(f"Class {cls}")
    plt.axis("off")
plt.tight_layout()
plt.show()

#<학습>
import os
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input

# ── 설정 ──
IMAGE_SIZE   = (224,224)
BATCH_SIZE   = 16
HEAD_EPOCHS  = 15
FINE_EPOCHS  = 25
INITIAL_LR   = 1e-4

TRAIN_DIR    = r"C:/Users/taedo/Downloads/intern_data/lumbar_spinal_dataset/data_preprocessed_labeled/training"
TEST_DIR     = r"C:/Users/taedo/Downloads/intern_data/lumbar_spinal_dataset/data_preprocessed_labeled/testing"

# ── 데이터 파이프라인 ──
def build_ds(path, augment=False, shuffle=False):
    ds = tf.keras.utils.image_dataset_from_directory(
        path,
        labels='inferred',
        label_mode='categorical',
        image_size=IMAGE_SIZE,
        batch_size=BATCH_SIZE,
        shuffle=shuffle
    )
    ds = ds.map(lambda x,y:(preprocess_input(x), y),
                num_parallel_calls=tf.data.AUTOTUNE)
    if augment:
        aug = tf.keras.Sequential([
            layers.RandomFlip('horizontal'),
            layers.RandomRotation(0.1),
            layers.RandomZoom(0.1),
            layers.RandomContrast(0.15),
        ])
        ds = ds.map(lambda x,y:(aug(x, training=True), y),
                    num_parallel_calls=tf.data.AUTOTUNE)
    return ds.prefetch(tf.data.AUTOTUNE)

train_ds = build_ds(TRAIN_DIR, augment=True, shuffle=True)
test_ds  = build_ds(TEST_DIR,  augment=False, shuffle=False)

# ── 모델 정의 ──
base = EfficientNetB0(include_top=False, weights='imagenet',
                      input_shape=(*IMAGE_SIZE,3))
base.trainable = False

inp = layers.Input((*IMAGE_SIZE,3))
x   = base(inp, training=False)
x   = layers.GlobalAveragePooling2D()(x)
x   = layers.BatchNormalization()(x)
x   = layers.Dropout(0.4)(x)
out = layers.Dense(train_ds.element_spec[1].shape[-1], activation='softmax')(x)
model = models.Model(inp, out)

# ── 콜백 ──
ckpt = callbacks.ModelCheckpoint(
    
    'best_full.weights.h5',
    monitor='val_loss',
    save_best_only=True,
    save_weights_only=True,
    verbose=1
)
es_head = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=1
)
rlr = callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    verbose=1
)

# ── 1) Head 학습 ──
model.compile(
    optimizer=optimizers.Adam(INITIAL_LR),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
model.fit(
    train_ds,
    epochs=HEAD_EPOCHS,
    callbacks=[ckpt, es_head, rlr],
    verbose=1
)

# 모델 적합할 때 train, valid로 나누기 비율은 9.5:0.5
# model.fit(
#     train_ds,
#     epochs=HEAD_EPOCHS,
#     validation_split=0.05,
#     callbacks=[ckpt, es_head, rlr],
#     verbose=1
# )

# ── 2) Fine‑tuning ──
base.trainable = True
for layer in base.layers[:-30]:
    layer.trainable = False

# EarlyStopping for fine-tuning on val_accuracy
es_fine = callbacks.EarlyStopping(
    monitor='accuracy',
    patience=3,
    restore_best_weights=True,
    verbose=1
)

model.compile(
    optimizer=optimizers.Adam(INITIAL_LR/10),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
model.fit(
    train_ds,
    epochs=FINE_EPOCHS,
    callbacks=[ckpt, rlr, es_fine],
    verbose=1
)

# ── 3) 최종 평가 ──
test_loss, test_acc = model.evaluate(test_ds, verbose=2)
print(f"▶ Final Test accuracy: {test_acc:.4f}")

from sklearn.metrics import confusion_matrix

true_classes = []
pred_classes = []

for images, labels in test_ds:
    # 모델이 각 이미지를 분류한 softmax 확률 값 받기
    preds = model.predict(images)
    # 실제 정답 라벨(one-hot vector)을 정수 클래스 번호로 변환
    true_classes.extend(np.argmax(labels.numpy(), axis=1))
    # 예측값 중 확률이 가장 높은 클래스를 클래스 번호로 선택
    pred_classes.extend(np.argmax(preds, axis=1))

# 배열 형태로 변환
true_classes = np.array(true_classes)
pred_classes = np.array(pred_classes)

cm = confusion_matrix(true_classes, pred_classes)

class_names = ['Thecal Sac', 'No Stenosis', 'Herniated Disc']
sensitivities = []
specificities = []

for i in range(len(class_names)):
    TP = cm[i,i]
    FN = cm[i,:].sum() - TP
    FP = cm[:,i].sum() - TP
    TN = cm.sum() - (TP + FP + FN)
    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0
    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0
    print(f"{class_names[i]} | Sensitivity: {sensitivity:.3f} | Specificity: {specificity:.3f}")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

cm = confusion_matrix(true_classes, pred_classes)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

TP_total = 0
FN_total = 0
FP_total = 0
TN_total = 0

for i in range(len(class_names)):
    TP = cm[i, i]
    FN = cm[i, :].sum() - TP
    FP = cm[:, i].sum() - TP
    TN = cm.sum() - (TP + FN + FP)
    TP_total += TP
    FN_total += FN
    FP_total += FP
    TN_total += TN

# 전체(마이크로) 민감도와 특이도 계산
micro_sensitivity = TP_total / (TP_total + FN_total) if (TP_total + FN_total) > 0 else 0
micro_specificity = TN_total / (TN_total + FP_total) if (TN_total + FP_total) > 0 else 0

print(f"▶ Accuracy: {test_acc:.4f}")
print(f"▶ Micro average Sensitivity: {micro_sensitivity:.3f}")
print(f"▶ Micro average Specificity: {micro_specificity:.3f}")

import numpy as np

pred_probs = []  # 모든 샘플의 클래스별 예측 확률
true_labels = [] # 실제 원-핫 레이블 (혹은 정수 인덱스)

for images, labels in test_ds:
    # 모델에 이미지를 넣어 softmax로 확률 예측 (배치별 반환)
    batch_probs = model.predict(images)
    pred_probs.append(batch_probs)
    true_labels.append(labels.numpy())

# 전체 샘플을 하나로 합치기
pred_probs = np.concatenate(pred_probs, axis=0)  # (전체 샘플, 클래스 수)
true_labels = np.concatenate(true_labels, axis=0)

# 실제 클래스와 예측 클래스(최대 확률 인덱스)
true_classes = np.argmax(true_labels, axis=1)  # 실제 클래스 (예: 0, 1, 2)
pred_classes = np.argmax(pred_probs, axis=1)   # 예측 클래스 (예: 0, 1, 2)

print(pred_probs.shape)       # (샘플 수, 클래스 수)
print(true_classes[:5])       # 실제 클래스 일부
print(pred_classes[:5])       # 예측 클래스 일부

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.preprocessing import label_binarize
import numpy as np

class_names = ['Thecal Sac', 'No Stenosis', 'Herniated Disc']  # 분류 클래스명
num_classes = len(class_names)

# true_classes: 실제 정답(0,1,2 등)
# pred_probs: 모델이 예측한 softmax 확률값 (샘플, 클래스)

# 진짜 레이블을 One-hot 벡터로 변환
true_onehot = label_binarize(true_classes, classes=np.arange(num_classes))

plt.figure(figsize=(8, 8))
for i, cls in enumerate(class_names):
    fpr, tpr, _ = roc_curve(true_onehot[:, i], pred_probs[:, i])
    auc_score = roc_auc_score(true_onehot[:, i], pred_probs[:, i])
    plt.plot(fpr, tpr, label=f'{cls} (AUC = {auc_score:.2f})')

plt.plot([0, 1], [0, 1], 'k--')  # 기준선
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()


